{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Imports\n",
    "'''\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor\n",
    "from IPython.display import clear_output, display\n",
    "import math\n",
    "from multiprocessing import  Pool\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "import shutil\n",
    "import string\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Util imports\n",
    "'''\n",
    "\n",
    "from utils.dataframeUtils import addComments, checkIfTrainCSVIsValid, cleanDataFrameFromNansandnans, countFakeNoFake, createIDLabelFile, createIDTitleCommentsTextLabelFile, createIDTitleTextLabelFile, createIDTitleFile, createIDTitleCommentsTextMetaDataLabelFile, createMetaDataLabelFile, encodeAuthors, replaceNanInScoreAndUpvote, show_pandas_n_front_columns, show_pandas_n_last_columns, writeAuthorListToCSV, writeOutCleanedDataFrameToCSV\n",
    "\n",
    "from utils.fileAndDirUtils import calcMeanAndStdOfImage, checkIfDirExistsAndCreate, checkIfImagesAreAvailableAndValid, checkIfImageIsAvaliable, copyImageFromAToB, listdir_fullpath, writeMeansToFile\n",
    "\n",
    "from utils.multiprocessingUtils import calculateMeansAndStdMultiprocessing, generateFileList, generateFileListForCopy, generateFileListForMeanAndStds, parallelize_dataframe, parallelize_dataframe_comments, resizeImagesMultiprocessing, resizeNormalizeImagesMultiprocessing, workerCopyAToB, workerMeanStds\n",
    "\n",
    "from utils.otherUtils import calcZeroBaseline, convertRowToDictionary, isBlank, parseStringAsNpArray, processComment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addComments1(dataframe):\n",
    "    try:\n",
    "        dataframe.insert(loc=dataframe.shape[1], column='comments', value=[list for i in range(dataframe.shape[0])])\n",
    "        dataframe.insert(loc=dataframe.shape[1], column='up_vote_comments', value=[list for i in range(dataframe.shape[0])])\n",
    "    except ValueError:\n",
    "        print('Found columns, ignoring inserting')\n",
    "    df_comments = pd.merge(dataframe, df_all_comments, left_on='id', right_on='submission_id', how='inner',suffixes=('_left','_right'))\n",
    "    for row in dataframe.itertuples(index=True, name=None):\n",
    "        row_dict = convertRowToDictionary(row, dataframe.columns, True)        \n",
    "        currentCommentsSelector = df_comments['submission_id'] == row_dict['id']\n",
    "\n",
    "        # Selecting all related comments and cleaning unnamed stuff\n",
    "        selectedComments = df_comments[currentCommentsSelector]\n",
    "        selectedComments = selectedComments.loc[:, ~selectedComments.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        clean_comments = []\n",
    "        clean_up_vote = []\n",
    "\n",
    "        if not selectedComments.empty:\n",
    "            if (len(selectedComments)) is not int(row_dict['num_comments']):\n",
    "                print(f'Checked comments and num_comments -> mismatch! len of comments found: {len(selectedComments)}, but should be {int(row_dict[\"num_comments\"])} at id {row_dict[\"id\"]}')\n",
    "                dataframe.at[row[0], 'num_comments'] = len(selectedComments)\n",
    "        \n",
    "            # Iterating over all found comments, cleaning them \n",
    "            for row_comment in selectedComments.itertuples(index=True, name=None):\n",
    "                row_dict_comments = convertRowToDictionary(row_comment, selectedComments.columns, True)\n",
    "                clean_comments.append(processComment(row_dict_comments['body']))\n",
    "                clean_up_vote.append(row_dict_comments['ups'])       \n",
    "        else:\n",
    "            dataframe.at[row[0], 'num_comments'] = 0\n",
    "        \n",
    "        # Inserting at correct position\n",
    "        dataframe.at[row[0], 'comments'] = clean_comments\n",
    "        dataframe.at[row[0], 'up_vote_comments'] = clean_up_vote\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def parallelize_dataframe_comments(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Settings\n",
    "'''\n",
    "\n",
    "# DecompressionBomb\n",
    "#https://pillow.readthedocs.io/en/5.1.x/releasenotes/5.0.0.html\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "# Settings Resize\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "IMG_SIZES_1 = (IMG_WIDTH,IMG_HEIGHT)\n",
    "IMG_WIDTH = 768\n",
    "IMG_HEIGHT = 768\n",
    "IMG_SIZES_2 = (IMG_WIDTH,IMG_HEIGHT)\n",
    "\n",
    "# Assign methods to all pandas dataframe calls  https://stackoverflow.com/questions/30608310/is-there-a-pandas-function-to-display-the-first-last-n-columns-as-in-head?noredirect=1&lq=1\n",
    "pd.DataFrame.show_pandas_n_front_columns = show_pandas_n_front_columns\n",
    "pd.DataFrame.show_pandas_n_last_columns = show_pandas_n_last_columns\n",
    "\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to all comments is: /home/armin/repos/FKD-Dataset/001_fakeddit_from_website/001_website_data/all_comments.tsv\n",
      "Path to train.tsv is: /home/armin/repos/FKD-Dataset/001_fakeddit_from_website/001_website_data/train.tsv\n",
      "Path to train images is: /home/armin/repos/FKD-Dataset/002_images/train\n",
      "Path to test.tsv is: /home/armin/repos/FKD-Dataset/001_fakeddit_from_website/001_website_data/test_public.tsv\n",
      "Path to test images is: /home/armin/repos/FKD-Dataset/002_images/test\n",
      "Path to val.tsv is: /home/armin/repos/FKD-Dataset/001_fakeddit_from_website/001_website_data/validate.tsv\n",
      "Path to test images is: /home/armin/repos/FKD-Dataset/002_images/val\n"
     ]
    }
   ],
   "source": [
    "path_to_fakeddit_dataset_dir_server = \"/home/armin/repos/FKD-Dataset\"\n",
    "path_to_fakeddit_dataset_dir_home = \"D:\\\\000_Diplomarbeit\\\\002_original_daten_bearbeitet\\\\000_Fakeddit\"\n",
    "path_to_all_gdrive_fakeddit_dataset_images = '/home/armin/repos/FKD-Dataset/001_fakeddit_from_website/003_gdrive_images'\n",
    "\n",
    "isServer = True\n",
    "\n",
    "if isServer:\n",
    "    path_to_fakeddit_dataset_dir = path_to_fakeddit_dataset_dir_server\n",
    "else:\n",
    "      path_to_fakeddit_dataset_dir = path_to_fakeddit_dataset_dir_home\n",
    "\n",
    "path_to_fakeddit_dataset_images_dir = os.path.join(path_to_fakeddit_dataset_dir , \"002_images\");\n",
    "\n",
    "\n",
    "# Path to comments TSV\n",
    "path_to_comments_tsv = os.path.join(path_to_fakeddit_dataset_dir , \"001_fakeddit_from_website\", \"001_website_data\", \"all_comments.tsv\")\n",
    "print(\"Path to all comments is: \" + path_to_comments_tsv)\n",
    "\n",
    "# Path to train data\n",
    "path_to_train_tsv = os.path.join(path_to_fakeddit_dataset_dir , \"001_fakeddit_from_website\", \"001_website_data\", \"train.tsv\")\n",
    "print(\"Path to train.tsv is: \" + path_to_train_tsv)\n",
    "\n",
    "path_to_train_images_dir = os.path.join(path_to_fakeddit_dataset_images_dir , \"train\")\n",
    "print(\"Path to train images is: \" + path_to_train_images_dir)\n",
    "\n",
    "# Path to test data\n",
    "path_to_test_tsv = os.path.join(path_to_fakeddit_dataset_dir , \"001_fakeddit_from_website\", \"001_website_data\", \"test_public.tsv\")\n",
    "print(\"Path to test.tsv is: \" + path_to_test_tsv)\n",
    "\n",
    "path_to_test_images_dir = os.path.join(path_to_fakeddit_dataset_images_dir , \"test\")\n",
    "print(\"Path to test images is: \" + path_to_test_images_dir)\n",
    "\n",
    "# Path to val data\n",
    "path_to_val_tsv = os.path.join(path_to_fakeddit_dataset_dir , \"001_fakeddit_from_website\", \"001_website_data\", \"validate.tsv\" )\n",
    "print(\"Path to val.tsv is: \" + path_to_val_tsv)\n",
    "\n",
    "path_to_val_images_dir = os.path.join(path_to_fakeddit_dataset_images_dir , \"val\")\n",
    "print(\"Path to test images is: \" + path_to_val_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armin/repos/fkd-preprocessing/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0,1,2,3,4,5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Excerpt from all comments\n",
    "df_all_comments = pd.read_csv(path_to_comments_tsv, header=0, sep='\\t')\n",
    "# df_all_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt from train set\n",
    "df_train_original = pd.read_csv(path_to_train_tsv, header=0, sep='\\t')\n",
    "df_train_original = df_train_original.loc[:, ~df_train_original.columns.str.contains('^Unnamed')]\n",
    "# df_train_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878218/878218 [00:34<00:00, 25567.19it/s]\n"
     ]
    }
   ],
   "source": [
    "checkIfDirExistsAndCreate(path_to_train_images_dir)  \n",
    "\n",
    "imageListTuple = generateFileListForCopy(df_train_original, path_to_all_gdrive_fakeddit_dataset_images, path_to_train_images_dir)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    results = list(tqdm(executor.map(workerCopyAToB, imageListTuple), total=len(imageListTuple)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting check directory, need to check: 639101 files... but have 878218 entries in dataframe\n",
      "Processed 325901 lines.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armin/repos/fkd-preprocessing/venv/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638506 images foundnes.\n",
      "595 images not found\n",
      "Check total images within data set, which are not available = 0\n",
      "Need to drop 239712 files, due to no images attached or not found images.\n"
     ]
    }
   ],
   "source": [
    "# Checking image dir if all images are available and structurally intact.\n",
    "\n",
    "temp_dataframe = df_train_original.copy()\n",
    "temp_keep_indices, temp_drop_indices = checkIfImagesAreAvailableAndValid(temp_dataframe, path_to_train_images_dir)\n",
    " \n",
    "temp_dataframe.describe()\n",
    "print(\"Need to drop \" + str(len(temp_drop_indices)) + \" files, due to no images attached or not found images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for nans\n",
      "Len of dataframe before 638506\n",
      "Len of dataframe after 560622\n",
      "original count 878218\n",
      "to drop count 239712\n",
      "len of new df 638506\n",
      "dropped checksum (must be 0!! = )0\n"
     ]
    }
   ],
   "source": [
    "# Filtering nans\n",
    "df_not_taken_train = df_train_original.take(temp_drop_indices)\n",
    "df_taken_train = df_train_original.take(temp_keep_indices)\n",
    "\n",
    "print('Checking for nans')\n",
    "print(f'Len of dataframe before {len(df_taken_train)}')\n",
    "df_taken_train = cleanDataFrameFromNansandnans(df_taken_train)\n",
    "print(f'Len of dataframe after {len(df_taken_train)}')\n",
    "\n",
    "\n",
    "print(\"original count \" + str(df_train_original.count()[\"id\"]))\n",
    "print(\"to drop count \" + str(len(temp_drop_indices)))\n",
    "print(\"len of new df \" + str(temp_dataframe.count()[\"id\"] - len(temp_drop_indices)))\n",
    "print(\"dropped checksum (must be 0!! = )\" + str(temp_dataframe.count()[\"id\"] - df_train_original.count()[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>2_way_label</th>\n",
       "      <th>3_way_label</th>\n",
       "      <th>6_way_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.606220e+05</td>\n",
       "      <td>393533.000000</td>\n",
       "      <td>560622.000000</td>\n",
       "      <td>393533.000000</td>\n",
       "      <td>560622.000000</td>\n",
       "      <td>560622.000000</td>\n",
       "      <td>560622.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.478669e+09</td>\n",
       "      <td>20.350748</td>\n",
       "      <td>395.478972</td>\n",
       "      <td>0.855417</td>\n",
       "      <td>0.393317</td>\n",
       "      <td>1.188769</td>\n",
       "      <td>1.885372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.970156e+07</td>\n",
       "      <td>141.389503</td>\n",
       "      <td>3051.310199</td>\n",
       "      <td>0.110086</td>\n",
       "      <td>0.488487</td>\n",
       "      <td>0.969418</td>\n",
       "      <td>1.782129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.212297e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-950.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.418107e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.488879e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.550452e+09</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.573859e+09</td>\n",
       "      <td>10783.000000</td>\n",
       "      <td>137179.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc   num_comments          score   upvote_ratio  \\\n",
       "count  5.606220e+05  393533.000000  560622.000000  393533.000000   \n",
       "mean   1.478669e+09      20.350748     395.478972       0.855417   \n",
       "std    6.970156e+07     141.389503    3051.310199       0.110086   \n",
       "min    1.212297e+09       0.000000    -950.000000       0.500000   \n",
       "25%    1.418107e+09       1.000000       5.000000       0.780000   \n",
       "50%    1.488879e+09       2.000000      14.000000       0.880000   \n",
       "75%    1.550452e+09       7.000000      46.000000       0.940000   \n",
       "max    1.573859e+09   10783.000000  137179.000000       1.000000   \n",
       "\n",
       "         2_way_label    3_way_label    6_way_label  \n",
       "count  560622.000000  560622.000000  560622.000000  \n",
       "mean        0.393317       1.188769       1.885372  \n",
       "std         0.488487       0.969418       1.782129  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000  \n",
       "50%         0.000000       2.000000       2.000000  \n",
       "75%         1.000000       2.000000       4.000000  \n",
       "max         1.000000       2.000000       5.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taken_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author                   28535\n",
       "clean_title                  0\n",
       "created_utc                  0\n",
       "domain                  167089\n",
       "hasImage                     0\n",
       "id                           0\n",
       "image_url                 1518\n",
       "linked_submission_id    393533\n",
       "num_comments            167089\n",
       "score                        0\n",
       "subreddit                    0\n",
       "title                        0\n",
       "upvote_ratio            167089\n",
       "2_way_label                  0\n",
       "3_way_label                  0\n",
       "6_way_label                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taken_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_cleaned_csv_file = os.path.join(path_to_fakeddit_dataset_dir, \"003_cleaned_datasets\", \"train_clean_meta_test.csv\")\n",
    "isTrainCSVValid = checkIfTrainCSVIsValid(path_to_cleaned_csv_file, df_taken_train)\n",
    "\n",
    "if isTrainCSVValid:\n",
    "    df_taken_train = pd.read_csv(path_to_cleaned_csv_file, header=0, sep='\\t')\n",
    "# Add mean and std column to df\n",
    "if not isTrainCSVValid:\n",
    "    df_taken_train.insert(loc=df_taken_train.shape[1], column='means', value=[list for i in range(df_taken_train.shape[0])])\n",
    "    df_taken_train.insert(loc=df_taken_train.shape[1], column='stds', value=[list for i in range(df_taken_train.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using already processes csv: False\n"
     ]
    }
   ],
   "source": [
    "print(f'Using already processes csv: {isTrainCSVValid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560622sed 870001 lines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 32964/560622 [02:27<40:59, 214.53it/s]  /home/armin/repos/fkd-preprocessing/venv/lib/python3.7/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "100%|██████████| 560622/560622 [42:27<00:00, 220.06it/s]  \n"
     ]
    }
   ],
   "source": [
    "if not isTrainCSVValid:\n",
    "    means, stds = calculateMeansAndStdMultiprocessing(df_taken_train, path_to_train_images_dir)\n",
    "    df_taken_train['means'] = means\n",
    "    df_taken_train['stds'] = stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df_taken_train = parallelize_dataframe(df_taken_train, addComments1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taken_train = replaceNanInScoreAndUpvote(df_taken_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores mean: 395.47897157086237, scores stds: 3051.3074771950264, num_comments mean: 9.194637384904624, num comments stds: 42.185042249523924 \n"
     ]
    }
   ],
   "source": [
    "# Calc z score normalization scores\n",
    "\n",
    "\n",
    "score_mean = np.nanmean(df_taken_train['score'])\n",
    "score_stds = np.nanstd(df_taken_train['score'])\n",
    "df_taken_train['score'] = (df_taken_train['score'] - score_mean) / score_stds\n",
    "\n",
    "num_comments_mean = np.nanmean(df_taken_train['num_comments'])\n",
    "num_comments_stds = np.nanstd(df_taken_train['num_comments'])\n",
    "\n",
    "df_taken_train['num_comments'] = (df_taken_train['num_comments'] - num_comments_mean) / num_comments_stds\n",
    "\n",
    "print(f'scores mean: {score_mean}, scores stds: {score_stds}, num_comments mean: {num_comments_mean}, num comments stds: {num_comments_stds} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_taken_train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_taken_train = replaceNanInScoreAndUpvote(df_taken_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir not found, creating /home/armin/repos/FKD-Dataset/010_configs instead\n",
      "Writing  file\n",
      "Means of dataset per channel is: [119.841705 112.0786   104.98751 ]\n"
     ]
    }
   ],
   "source": [
    "if not isTrainCSVValid:\n",
    "    meansOfDataset = np.mean(np.array(means), axis=0)\n",
    "    pathToMeansDir = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\")\n",
    "    pathToMeansFile = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\", 'means_non_resized.txt')\n",
    "    checkIfDirExistsAndCreate(pathToMeansDir)\n",
    "    writeMeansToFile(str(meansOfDataset), pathToMeansFile)\n",
    "    print(f'Means of dataset per channel is: {meansOfDataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing cleaned dataframe -> \n",
      "no outdir found, creating it instead!\n",
      "finished writing cleaned dataframe!\n"
     ]
    }
   ],
   "source": [
    "if not isTrainCSVValid:\n",
    "    path_to_cleaned_files = os.path.join(path_to_fakeddit_dataset_dir, \"003_cleaned_datasets\")\n",
    "    df_taken_train = writeOutCleanedDataFrameToCSV(df_taken_train, path_to_cleaned_files, \"train_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt from test set\n",
    "df_test_original = pd.read_csv(path_to_test_tsv, header=0, sep='\\t')\n",
    "df_test_original = df_test_original.loc[:, ~df_test_original.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92444/92444 [00:03<00:00, 24665.96it/s]\n"
     ]
    }
   ],
   "source": [
    "checkIfDirExistsAndCreate(path_to_test_images_dir)  \n",
    "\n",
    "imageListTuple = generateFileListForCopy(df_test_original, path_to_all_gdrive_fakeddit_dataset_images, path_to_test_images_dir)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    results = list(tqdm(executor.map(workerCopyAToB, imageListTuple), total=len(imageListTuple)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting check directory, need to check: 67255 files... but have 92444 entries in dataframe\n",
      "67185 images foundnes.\n",
      "70 images not found\n",
      "Check total images within data set, which are not available = 0\n",
      "Need to drop 25259 files, due to no images attached or not found images.\n"
     ]
    }
   ],
   "source": [
    "temp_dataframe = df_test_original.copy()\n",
    "temp_keep_indices, temp_drop_indices = checkIfImagesAreAvailableAndValid(temp_dataframe, path_to_test_images_dir)\n",
    " \n",
    "temp_dataframe.describe()\n",
    "print(\"Need to drop \" + str(len(temp_drop_indices)) + \" files, due to no images attached or not found images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for nans\n",
      "Len of dataframe before 67185\n",
      "Len of dataframe after 58954\n",
      "original count 92444\n",
      "to drop count 25259\n",
      "len of new df 67185\n",
      "dropped checksum (must be 0!! = )0\n",
      "writing cleaned dataframe -> \n",
      "finished writing cleaned dataframe!\n"
     ]
    }
   ],
   "source": [
    "df_not_taken_test = df_test_original.take(temp_drop_indices)\n",
    "df_taken_test = df_test_original.take(temp_keep_indices)\n",
    "\n",
    "\n",
    "print('Checking for nans')\n",
    "print(f'Len of dataframe before {len(df_taken_test)}')\n",
    "df_taken_test = cleanDataFrameFromNansandnans(df_taken_test)\n",
    "print(f'Len of dataframe after {len(df_taken_test)}')\n",
    "\n",
    "\n",
    "print(\"original count \" + str(df_test_original.count()[\"id\"]))\n",
    "print(\"to drop count \" + str(len(temp_drop_indices)))\n",
    "print(\"len of new df \" + str(temp_dataframe.count()[\"id\"] - len(temp_drop_indices)))\n",
    "print(\"dropped checksum (must be 0!! = )\" + str(temp_dataframe.count()[\"id\"] - df_test_original.count()[\"id\"]))\n",
    "\n",
    "\n",
    "\n",
    "path_to_cleaned_files = os.path.join(path_to_fakeddit_dataset_dir, \"003_cleaned_datasets\")\n",
    "df_taken_test = writeOutCleanedDataFrameToCSV(df_taken_test, path_to_cleaned_files, \"test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df_taken_test = parallelize_dataframe(df_taken_test, addComments1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taken_test = replaceNanInScoreAndUpvote(df_taken_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taken_test['score'] = (df_taken_test['score'] - score_mean) / score_stds\n",
    "df_taken_test['num_comments'] = (df_taken_test['num_comments'] - num_comments_mean) / num_comments_stds\n",
    "# df_taken_test.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excerpt from val set\n",
    "df_val_original = pd.read_csv(path_to_val_tsv, header=0, sep='\\t')\n",
    "df_val_original = df_val_original.loc[:, ~df_val_original.columns.str.contains('^Unnamed')]\n",
    "df_val_original['title'] = df_val_original['title'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92444/92444 [00:03<00:00, 25662.26it/s]\n"
     ]
    }
   ],
   "source": [
    "checkIfDirExistsAndCreate(path_to_val_images_dir)  \n",
    "\n",
    "imageListTuple = generateFileListForCopy(df_val_original, path_to_all_gdrive_fakeddit_dataset_images, path_to_val_images_dir)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    results = list(tqdm(executor.map(workerCopyAToB, imageListTuple), total=len(imageListTuple)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting check directory, need to check: 67208 files... but have 92444 entries in dataframe\n",
      "67140 images foundnes.\n",
      "68 images not found\n",
      "Check total images within data set, which are not available = 0\n",
      "Need to drop 25304 files, due to no images attached or not found images.\n"
     ]
    }
   ],
   "source": [
    "temp_dataframe = df_val_original.copy()\n",
    "temp_keep_indices, temp_drop_indices = checkIfImagesAreAvailableAndValid(temp_dataframe, path_to_val_images_dir)\n",
    " \n",
    "temp_dataframe.describe()\n",
    "print(\"Need to drop \" + str(len(temp_drop_indices)) + \" files, due to no images attached or not found images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for nans\n",
      "Len of dataframe before 67140\n",
      "Len of dataframe after 58972\n",
      "original count 92444\n",
      "to drop count 25304\n",
      "len of new df 67140\n",
      "dropped checksum (must be 0!! = )0\n",
      "TODO: Chack and raise error if not 0\n",
      "writing cleaned dataframe -> \n",
      "finished writing cleaned dataframe!\n"
     ]
    }
   ],
   "source": [
    "# Take ist schneller als drop!\n",
    "df_not_taken_val = df_val_original.take(temp_drop_indices)\n",
    "df_taken_val = df_val_original.take(temp_keep_indices)\n",
    "\n",
    "print('Checking for nans')\n",
    "print(f'Len of dataframe before {len(df_taken_val)}')\n",
    "df_taken_val = cleanDataFrameFromNansandnans(df_taken_val)\n",
    "print(f'Len of dataframe after {len(df_taken_val)}')\n",
    "\n",
    "\n",
    "\n",
    "print(\"original count \" + str(df_val_original.count()[\"id\"]))\n",
    "print(\"to drop count \" + str(len(temp_drop_indices)))\n",
    "print(\"len of new df \" + str(temp_dataframe.count()[\"id\"] - len(temp_drop_indices)))\n",
    "print(\"dropped checksum (must be 0!! = )\" + str(temp_dataframe.count()[\"id\"] - df_val_original.count()[\"id\"]))\n",
    "\n",
    "print(\"TODO: Chack and raise error if not 0\")\n",
    "\n",
    "\n",
    "\n",
    "path_to_cleaned_files = os.path.join(path_to_fakeddit_dataset_dir, \"003_cleaned_datasets\")\n",
    "\n",
    "df_taken_val = writeOutCleanedDataFrameToCSV(df_taken_val, path_to_cleaned_files, \"val_clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df_taken_val = parallelize_dataframe(df_taken_val, addComments1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taken_val = replaceNanInScoreAndUpvote(df_taken_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taken_val['score'] = (df_taken_val['score'] - score_mean) / score_stds\n",
    "df_taken_val['num_comments'] = (df_taken_val['num_comments'] - num_comments_mean) / num_comments_stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_authors_train = df_taken_train.author.unique().tolist()\n",
    "all_authors_test = df_taken_test.author.unique().tolist()\n",
    "all_authors_val = df_taken_val.author.unique().tolist()\n",
    "all_authors_noset = all_authors_train +  all_authors_test  + all_authors_val\n",
    "all_authors = list(set(all_authors_noset))\n",
    "\n",
    "\n",
    "all_authors.append('no_author')\n",
    "pathToAuthorDir = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\")\n",
    "if not isTrainCSVValid:   \n",
    "    pathToAuthorFile_train = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\", 'all_authors_train.csv')\n",
    "    checkIfDirExistsAndCreate(pathToAuthorDir)   \n",
    "    writeAuthorListToCSV(all_authors_train, pathToAuthorFile_train)\n",
    "    df_taken_train = encodeAuthors(df_taken_train, all_authors)\n",
    "\n",
    "pathToAuthorFile_noset = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\", 'all_authors_noset.csv')\n",
    "writeAuthorListToCSV(all_authors_noset, pathToAuthorFile_noset)\n",
    "pathToAuthorFile_test = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\", 'all_authors_test.csv')\n",
    "writeAuthorListToCSV(all_authors_test, pathToAuthorFile_test)\n",
    "pathToAuthorFile_val = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\", 'all_authors_val.csv')   \n",
    "writeAuthorListToCSV(all_authors_val, pathToAuthorFile_val)\n",
    "\n",
    "pathToAuthorFile_all = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\", 'all_authors.csv')\n",
    "writeAuthorListToCSV(all_authors, pathToAuthorFile_all)\n",
    "df_taken_test = encodeAuthors(df_taken_test, all_authors)\n",
    "df_taken_val = encodeAuthors(df_taken_val, all_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_authors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-42b1ffc526ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_authors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_authors' is not defined"
     ]
    }
   ],
   "source": [
    "len(all_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning is done -> doing some statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By removing all comments without images and images which are not found, we have 607869 samples for training left\n",
      "\n",
      "Training set total -> 878218 we took 560622 samples  because of missing images or only text modality available.\n",
      "\n",
      "This is 64.0 % of the whole train set\n",
      "\n",
      "We have 340120 not fakes!\n",
      "We have 220502 fakes!\n",
      "We have 340120 true labels and 220502 fakes.\n",
      "The zero baseline for this set is: 61%. \n",
      "\n",
      "By removing all comments without images and images which are not found, we have 63917 samples for testing left\n",
      "\n",
      "Test set total -> 76752 we took 58954 samples  because of missing images or only text modality available.\n",
      "\n",
      "This is 64.0 % of the whole test set\n",
      "\n",
      "We have 35608 not fakes!\n",
      "We have 23346 fakes!\n",
      "We have 35608 true labels and 23346 fakes.\n",
      "The zero baseline for this set is: 60%. \n",
      "\n",
      "By removing all comments without images and images which are not found, we have 63865 samples for validating left\n",
      "\n",
      "Validation set total -> 76767 we took 58972 samples  because of missing images or only text modality available.\n",
      "\n",
      "This is 77.0 % of the whole validation set\n",
      "\n",
      "We have 35816 not fakes!\n",
      "We have 23156 fakes!\n",
      "We have 35816 true labels and 23156 fakes.\n",
      "The zero baseline for this set is: 61%. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"By removing all comments without images and images which are not found, we have \" + str(df_train_original.count()[0] - df_not_taken_train.count()[0]) + \" samples for training left\")\n",
    "print()\n",
    "print(\"Training set total -> \" + str(df_train_original.count()['id']) + \" we took \" + str(df_taken_train.count()['id']) + \" samples  because of missing images or only text modality available.\")\n",
    "print()\n",
    "percentage_train = ( df_taken_train.count()['id'] * 100) /   df_train_original.count()['id']\n",
    "print(\"This is \" + str(round(percentage_train)) + \" % of the whole train set\")\n",
    "print()\n",
    "count_fake, count_not_fake = countFakeNoFake(df_taken_train)\n",
    "print(\"We have \" + str(count_not_fake) + \" true labels and \" + str(count_fake) + \" fakes.\")\n",
    "calcZeroBaseline(count_fake, count_not_fake) \n",
    "print()\n",
    "\n",
    "\n",
    "print(\"By removing all comments without images and images which are not found, we have \" + str(df_test_original.count()[0] - df_not_taken_test.count()[0]) + \" samples for testing left\")\n",
    "print()\n",
    "print(\"Test set total -> \" + str(df_test_original.count()[0]) + \" we took \" + str(df_taken_test.count()[0]) + \" samples  because of missing images or only text modality available.\")\n",
    "print()\n",
    "percentage_test = ( df_taken_test.count()['id'] * 100) /   df_test_original.count()['id']\n",
    "print(\"This is \" + str(round(percentage_test)) + \" % of the whole test set\")\n",
    "print()\n",
    "count_fake, count_not_fake = countFakeNoFake(df_taken_test)\n",
    "print(\"We have \" + str(count_not_fake) + \" true labels and \" + str(count_fake) + \" fakes.\")\n",
    "calcZeroBaseline(count_fake, count_not_fake)\n",
    "# print()\n",
    "\n",
    "print()\n",
    "print(\"By removing all comments without images and images which are not found, we have \" + str(df_val_original.count()[0] - df_not_taken_val.count()[0]) + \" samples for validating left\")\n",
    "print()\n",
    "print(\"Validation set total -> \" + str(df_val_original.count()[0]) + \" we took \" + str(df_taken_val.count()[0]) + \" samples  because of missing images or only text modality available.\")\n",
    "print()\n",
    "percentage_val = ( df_taken_val.count()[0] * 100) /   df_val_original.count()[0]\n",
    "print(\"This is \" + str(round(percentage_val)) + \" % of the whole validation set\")\n",
    "print()\n",
    "count_fake, count_not_fake = countFakeNoFake(df_taken_val)\n",
    "print(\"We have \" + str(count_not_fake) + \" true labels and \" + str(count_fake) + \" fakes.\")\n",
    "calcZeroBaseline(count_fake, count_not_fake)  \n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for easier handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no outdir found, creating it instead!\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "path_to_cleaned_files = os.path.join(path_to_fakeddit_dataset_dir, \"004_images_id_label_files\")\n",
    "df_train_labels = createIDLabelFile(df_taken_train, path_to_cleaned_files, \"train_id_label.csv\", True)\n",
    "\n",
    "# test\n",
    "path_to_cleaned_files = os.path.join(path_to_fakeddit_dataset_dir, \"004_images_id_label_files\")\n",
    "df_test_labels = createIDLabelFile(df_taken_test, path_to_cleaned_files, \"test_id_label.csv\")\n",
    "\n",
    "# val\n",
    "path_to_cleaned_files = os.path.join(path_to_fakeddit_dataset_dir, \"004_images_id_label_files\")\n",
    "df_val_labels = createIDLabelFile(df_taken_val, path_to_cleaned_files, \"val_id_label.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no outdir found, creating it instead!\n"
     ]
    }
   ],
   "source": [
    "# Creating text id label files\n",
    "\n",
    "#train\n",
    "path_to_cleaned_files = os.path.join(path_to_fakeddit_dataset_dir, \"005_text_id_label_files\")\n",
    "createIDTitleFile(df_taken_train, path_to_cleaned_files , 'train_id_text_label.csv')\n",
    "\n",
    "#test\n",
    "path_to_cleaned_files = os.path.join(path_to_fakeddit_dataset_dir, \"005_text_id_label_files\")\n",
    "createIDTitleFile(df_taken_test, path_to_cleaned_files , 'test_id_text_label.csv')\n",
    "\n",
    "#val\n",
    "path_to_cleaned_files = os.path.join(path_to_fakeddit_dataset_dir, \"005_text_id_label_files\")\n",
    "createIDTitleFile(df_taken_val, path_to_cleaned_files , 'val_id_text_label.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir not found, creating /home/armin/repos/FKD-Dataset/006_images_resized_2/train instead\n",
      "Processed 560601 lines.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560622/560622 [00:08<00:00, 62431.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 560601 lines.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560622/560622 [1:36:41<00:00, 96.63it/s]  \n"
     ]
    }
   ],
   "source": [
    "pathToAllImages = os.path.join(path_to_fakeddit_dataset_dir, \"002_images\")\n",
    "pathToAllLabels = os.path.join(path_to_fakeddit_dataset_dir, \"004_images_id_label_files\")\n",
    "pathToAllResizedImages = os.path.join(path_to_fakeddit_dataset_dir, \"006_images_resized\")\n",
    "pathToAllResizedImages_2 = os.path.join(path_to_fakeddit_dataset_dir, \"006_images_resized_2\")\n",
    "\n",
    "\n",
    "# Train Set Handling\n",
    "pathToSourceTrainImages = os.path.join(pathToAllImages, \"train\")\n",
    "pathToTrainLabels = os.path.join(pathToAllLabels, \"train_id_label.csv\")\n",
    "pathToDestTrainImages = os.path.join(pathToAllResizedImages, \"train\")\n",
    "pathToDestTrainImages_2 = os.path.join(pathToAllResizedImages_2, \"train\")\n",
    "checkIfDirExistsAndCreate(pathToDestTrainImages)\n",
    "checkIfDirExistsAndCreate(pathToDestTrainImages_2)\n",
    "imageListTuple = generateFileList(df_train_labels, pathToSourceTrainImages, pathToDestTrainImages, IMG_SIZES_1)\n",
    "\n",
    "resizeImagesMultiprocessing(imageListTuple)\n",
    "\n",
    "imageListTuple = generateFileList(df_train_labels, pathToSourceTrainImages, pathToDestTrainImages_2, IMG_SIZES_2)\n",
    "\n",
    "resizeImagesMultiprocessing(imageListTuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560622sed 560001 lines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560622/560622 [07:09<00:00, 1306.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# df_taken_train.insert(loc=df_taken_train.shape[1], column='means_resized', value=[list for i in range(df_taken_train.shape[0])])\n",
    "# df_taken_train.insert(loc=df_taken_train.shape[1], column='stds_resized', value=[list for i in range(df_taken_train.shape[0])])\n",
    "\n",
    "means, stds = calculateMeansAndStdMultiprocessing(df_taken_train, pathToDestTrainImages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing  file\n",
      "Writing  file\n",
      "Means of dataset per channel is: [119.80977 112.05151 104.99459]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meansOfDataset = np.mean(np.array(means), axis=0)\n",
    "pathToMeansDir = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\")\n",
    "pathToMeansFile = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\", 'means_resized.txt')\n",
    "pathToStdsFile = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\", 'stds_resized.txt')\n",
    "\n",
    "checkIfDirExistsAndCreate(pathToMeansDir)\n",
    "\n",
    "writeMeansToFile(str(meansOfDataset), pathToMeansFile)\n",
    "writeMeansToFile(str(stds), pathToStdsFile)\n",
    "print(f'Means of dataset per channel is: {meansOfDataset}')\n",
    "print(f'Means of dataset per channel is: {stds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560622sed 560001 lines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560622/560622 [43:54<00:00, 212.81it/s]  \n"
     ]
    }
   ],
   "source": [
    "means, stds = calculateMeansAndStdMultiprocessing(df_taken_train, pathToDestTrainImages_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing  file\n",
      "Writing  file\n",
      "Means of dataset per channel is: [119.80394  112.04619  105.001976]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meansOfDataset = np.mean(np.array(means), axis=0)\n",
    "pathToMeansDir = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\")\n",
    "pathToMeansFile = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\", 'means_resized_768.txt')\n",
    "pathToStdsFile = os.path.join(path_to_fakeddit_dataset_dir, \"010_configs\", 'stds_resized_768.txt')\n",
    "\n",
    "checkIfDirExistsAndCreate(pathToMeansDir)\n",
    "\n",
    "writeMeansToFile(str(meansOfDataset), pathToMeansFile)\n",
    "writeMeansToFile(str(stds), pathToStdsFile)\n",
    "print(f'Means of dataset per channel is: {meansOfDataset}')\n",
    "print(f'Means of dataset per channel is: {stds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir not found, creating /home/armin/repos/FKD-Dataset/006_images_resized_2/test instead\n",
      "Processed 1 lines.\r",
      "Processed 101 lines.\r",
      "Processed 201 lines.\r",
      "Processed 301 lines.\r",
      "Processed 401 lines.\r",
      "Processed 501 lines.\r",
      "Processed 601 lines.\r",
      "Processed 701 lines.\r",
      "Processed 801 lines.\r",
      "Processed 901 lines.\r",
      "Processed 1001 lines.\r",
      "Processed 1101 lines.\r",
      "Processed 1201 lines.\r",
      "Processed 1301 lines.\r",
      "Processed 1401 lines.\r",
      "Processed 1501 lines.\r",
      "Processed 1601 lines.\r",
      "Processed 1701 lines.\r",
      "Processed 1801 lines.\r",
      "Processed 1901 lines.\r",
      "Processed 2001 lines.\r",
      "Processed 2101 lines.\r",
      "Processed 2201 lines.\r",
      "Processed 2301 lines.\r",
      "Processed 2401 lines.\r",
      "Processed 2501 lines.\r",
      "Processed 2601 lines.\r",
      "Processed 2701 lines.\r",
      "Processed 2801 lines.\r",
      "Processed 2901 lines.\r",
      "Processed 3001 lines.\r",
      "Processed 3101 lines.\r",
      "Processed 3201 lines.\r",
      "Processed 3301 lines.\r",
      "Processed 3401 lines.\r",
      "Processed 3501 lines.\r",
      "Processed 3601 lines.\r",
      "Processed 3701 lines.\r",
      "Processed 3801 lines.\r",
      "Processed 3901 lines.\r",
      "Processed 4001 lines.\r",
      "Processed 4101 lines.\r",
      "Processed 4201 lines.\r",
      "Processed 4301 lines.\r",
      "Processed 4401 lines.\r",
      "Processed 4501 lines.\r",
      "Processed 4601 lines.\r",
      "Processed 4701 lines.\r",
      "Processed 4801 lines.\r",
      "Processed 4901 lines.\r",
      "Processed 5001 lines.\r",
      "Processed 5101 lines.\r",
      "Processed 5201 lines.\r",
      "Processed 5301 lines.\r",
      "Processed 5401 lines.\r",
      "Processed 5501 lines.\r",
      "Processed 5601 lines.\r",
      "Processed 5701 lines.\r",
      "Processed 5801 lines.\r",
      "Processed 5901 lines.\r",
      "Processed 6001 lines.\r",
      "Processed 6101 lines.\r",
      "Processed 6201 lines.\r",
      "Processed 6301 lines.\r",
      "Processed 6401 lines.\r",
      "Processed 6501 lines.\r",
      "Processed 6601 lines.\r",
      "Processed 6701 lines.\r",
      "Processed 6801 lines.\r",
      "Processed 6901 lines.\r",
      "Processed 7001 lines.\r",
      "Processed 7101 lines.\r",
      "Processed 7201 lines.\r",
      "Processed 7301 lines.\r",
      "Processed 7401 lines.\r",
      "Processed 7501 lines.\r",
      "Processed 7601 lines.\r",
      "Processed 7701 lines.\r",
      "Processed 7801 lines.\r",
      "Processed 7901 lines.\r",
      "Processed 8001 lines.\r",
      "Processed 8101 lines.\r",
      "Processed 8201 lines.\r",
      "Processed 8301 lines.\r",
      "Processed 8401 lines.\r",
      "Processed 8501 lines.\r",
      "Processed 8601 lines.\r",
      "Processed 8701 lines.\r",
      "Processed 8801 lines.\r",
      "Processed 8901 lines.\r",
      "Processed 9001 lines.\r",
      "Processed 9101 lines.\r",
      "Processed 9201 lines.\r",
      "Processed 9301 lines.\r",
      "Processed 9401 lines.\r",
      "Processed 9501 lines.\r",
      "Processed 9601 lines.\r",
      "Processed 9701 lines.\r",
      "Processed 9801 lines.\r",
      "Processed 9901 lines.\r",
      "Processed 10001 lines.\r",
      "Processed 10101 lines.\r",
      "Processed 10201 lines.\r",
      "Processed 10301 lines.\r",
      "Processed 10401 lines.\r",
      "Processed 10501 lines.\r",
      "Processed 10601 lines.\r",
      "Processed 10701 lines.\r",
      "Processed 10801 lines.\r",
      "Processed 10901 lines.\r",
      "Processed 11001 lines.\r",
      "Processed 11101 lines.\r",
      "Processed 11201 lines.\r",
      "Processed 11301 lines.\r",
      "Processed 11401 lines.\r",
      "Processed 11501 lines.\r",
      "Processed 11601 lines.\r",
      "Processed 11701 lines.\r",
      "Processed 11801 lines.\r",
      "Processed 11901 lines.\r",
      "Processed 12001 lines.\r",
      "Processed 12101 lines.\r",
      "Processed 12201 lines.\r",
      "Processed 12301 lines.\r",
      "Processed 12401 lines.\r",
      "Processed 12501 lines.\r",
      "Processed 12601 lines.\r",
      "Processed 12701 lines.\r",
      "Processed 12801 lines.\r",
      "Processed 12901 lines.\r",
      "Processed 13001 lines.\r",
      "Processed 13101 lines.\r",
      "Processed 13201 lines.\r",
      "Processed 13301 lines.\r",
      "Processed 13401 lines.\r",
      "Processed 13501 lines.\r",
      "Processed 13601 lines.\r",
      "Processed 13701 lines.\r",
      "Processed 13801 lines.\r",
      "Processed 13901 lines.\r",
      "Processed 14001 lines.\r",
      "Processed 14101 lines.\r",
      "Processed 14201 lines.\r",
      "Processed 14301 lines.\r",
      "Processed 14401 lines.\r",
      "Processed 14501 lines.\r",
      "Processed 14601 lines.\r",
      "Processed 14701 lines.\r",
      "Processed 14801 lines.\r",
      "Processed 14901 lines.\r",
      "Processed 15001 lines.\r",
      "Processed 15101 lines.\r",
      "Processed 15201 lines.\r",
      "Processed 15301 lines.\r",
      "Processed 15401 lines.\r",
      "Processed 15501 lines.\r",
      "Processed 15601 lines.\r",
      "Processed 15701 lines.\r",
      "Processed 15801 lines.\r",
      "Processed 15901 lines.\r",
      "Processed 16001 lines.\r",
      "Processed 16101 lines.\r",
      "Processed 16201 lines.\r",
      "Processed 16301 lines.\r",
      "Processed 16401 lines.\r",
      "Processed 16501 lines.\r",
      "Processed 16601 lines.\r",
      "Processed 16701 lines.\r",
      "Processed 16801 lines.\r",
      "Processed 16901 lines.\r",
      "Processed 17001 lines.\r",
      "Processed 17101 lines.\r",
      "Processed 17201 lines.\r",
      "Processed 17301 lines.\r",
      "Processed 17401 lines.\r",
      "Processed 17501 lines.\r",
      "Processed 17601 lines.\r",
      "Processed 17701 lines.\r",
      "Processed 17801 lines.\r",
      "Processed 17901 lines.\r",
      "Processed 18001 lines.\r",
      "Processed 18101 lines.\r",
      "Processed 18201 lines.\r",
      "Processed 18301 lines.\r",
      "Processed 18401 lines.\r",
      "Processed 18501 lines.\r",
      "Processed 18601 lines.\r",
      "Processed 18701 lines.\r",
      "Processed 18801 lines.\r",
      "Processed 18901 lines.\r",
      "Processed 19001 lines.\r",
      "Processed 19101 lines.\r",
      "Processed 19201 lines.\r",
      "Processed 19301 lines.\r",
      "Processed 19401 lines.\r",
      "Processed 19501 lines.\r",
      "Processed 19601 lines.\r",
      "Processed 19701 lines.\r",
      "Processed 19801 lines.\r",
      "Processed 19901 lines.\r",
      "Processed 20001 lines.\r",
      "Processed 20101 lines.\r",
      "Processed 20201 lines.\r",
      "Processed 20301 lines.\r",
      "Processed 20401 lines.\r",
      "Processed 20501 lines.\r",
      "Processed 20601 lines.\r",
      "Processed 20701 lines.\r",
      "Processed 20801 lines.\r",
      "Processed 20901 lines.\r",
      "Processed 21001 lines.\r",
      "Processed 21101 lines.\r",
      "Processed 21201 lines.\r",
      "Processed 21301 lines.\r",
      "Processed 21401 lines.\r",
      "Processed 21501 lines.\r",
      "Processed 21601 lines.\r",
      "Processed 21701 lines.\r",
      "Processed 21801 lines.\r",
      "Processed 21901 lines.\r",
      "Processed 22001 lines.\r",
      "Processed 22101 lines.\r",
      "Processed 22201 lines.\r",
      "Processed 22301 lines.\r",
      "Processed 22401 lines.\r",
      "Processed 22501 lines.\r",
      "Processed 22601 lines.\r",
      "Processed 22701 lines.\r",
      "Processed 22801 lines.\r",
      "Processed 22901 lines.\r",
      "Processed 23001 lines.\r",
      "Processed 23101 lines.\r",
      "Processed 23201 lines.\r",
      "Processed 23301 lines.\r",
      "Processed 23401 lines.\r",
      "Processed 23501 lines.\r",
      "Processed 23601 lines.\r",
      "Processed 23701 lines.\r",
      "Processed 23801 lines.\r",
      "Processed 23901 lines.\r",
      "Processed 24001 lines.\r",
      "Processed 24101 lines.\r",
      "Processed 24201 lines.\r",
      "Processed 24301 lines.\r",
      "Processed 24401 lines.\r",
      "Processed 24501 lines.\r",
      "Processed 24601 lines.\r",
      "Processed 24701 lines.\r",
      "Processed 24801 lines.\r",
      "Processed 24901 lines.\r",
      "Processed 25001 lines.\r",
      "Processed 25101 lines.\r",
      "Processed 25201 lines.\r",
      "Processed 25301 lines.\r",
      "Processed 25401 lines.\r",
      "Processed 25501 lines.\r",
      "Processed 25601 lines.\r",
      "Processed 25701 lines.\r",
      "Processed 25801 lines.\r",
      "Processed 25901 lines.\r",
      "Processed 26001 lines.\r",
      "Processed 26101 lines.\r",
      "Processed 26201 lines.\r",
      "Processed 26301 lines.\r",
      "Processed 26401 lines.\r",
      "Processed 26501 lines.\r",
      "Processed 26601 lines.\r",
      "Processed 26701 lines.\r",
      "Processed 26801 lines.\r",
      "Processed 26901 lines.\r",
      "Processed 27001 lines.\r",
      "Processed 27101 lines.\r",
      "Processed 27201 lines.\r",
      "Processed 27301 lines.\r",
      "Processed 27401 lines.\r",
      "Processed 27501 lines.\r",
      "Processed 27601 lines.\r",
      "Processed 27701 lines.\r",
      "Processed 27801 lines.\r",
      "Processed 27901 lines.\r",
      "Processed 28001 lines.\r",
      "Processed 28101 lines.\r",
      "Processed 28201 lines.\r",
      "Processed 28301 lines.\r",
      "Processed 28401 lines.\r",
      "Processed 28501 lines.\r",
      "Processed 28601 lines.\r",
      "Processed 28701 lines.\r",
      "Processed 28801 lines.\r",
      "Processed 28901 lines.\r",
      "Processed 29001 lines.\r",
      "Processed 29101 lines.\r",
      "Processed 29201 lines.\r",
      "Processed 29301 lines.\r",
      "Processed 29401 lines.\r",
      "Processed 29501 lines.\r",
      "Processed 29601 lines.\r",
      "Processed 29701 lines.\r",
      "Processed 29801 lines.\r",
      "Processed 29901 lines.\r",
      "Processed 30001 lines.\r",
      "Processed 30101 lines.\r",
      "Processed 30201 lines.\r",
      "Processed 30301 lines.\r",
      "Processed 30401 lines.\r",
      "Processed 30501 lines.\r",
      "Processed 30601 lines.\r",
      "Processed 30701 lines.\r",
      "Processed 30801 lines.\r",
      "Processed 30901 lines.\r",
      "Processed 31001 lines.\r",
      "Processed 31101 lines.\r",
      "Processed 31201 lines.\r",
      "Processed 31301 lines.\r",
      "Processed 31401 lines.\r",
      "Processed 31501 lines.\r",
      "Processed 31601 lines.\r",
      "Processed 31701 lines.\r",
      "Processed 31801 lines.\r",
      "Processed 31901 lines.\r",
      "Processed 32001 lines.\r",
      "Processed 32101 lines.\r",
      "Processed 32201 lines.\r",
      "Processed 32301 lines.\r",
      "Processed 32401 lines.\r",
      "Processed 32501 lines.\r",
      "Processed 32601 lines.\r",
      "Processed 32701 lines.\r",
      "Processed 32801 lines.\r",
      "Processed 32901 lines.\r",
      "Processed 33001 lines.\r",
      "Processed 33101 lines.\r",
      "Processed 33201 lines.\r",
      "Processed 33301 lines.\r",
      "Processed 33401 lines.\r",
      "Processed 33501 lines.\r",
      "Processed 33601 lines.\r",
      "Processed 33701 lines.\r",
      "Processed 33801 lines.\r",
      "Processed 33901 lines.\r",
      "Processed 34001 lines.\r",
      "Processed 34101 lines.\r",
      "Processed 34201 lines.\r",
      "Processed 34301 lines.\r",
      "Processed 34401 lines.\r",
      "Processed 34501 lines.\r",
      "Processed 34601 lines.\r",
      "Processed 34701 lines.\r",
      "Processed 34801 lines.\r",
      "Processed 34901 lines.\r",
      "Processed 35001 lines.\r",
      "Processed 35101 lines.\r",
      "Processed 35201 lines.\r",
      "Processed 35301 lines.\r",
      "Processed 35401 lines.\r",
      "Processed 35501 lines.\r",
      "Processed 35601 lines.\r",
      "Processed 35701 lines.\r",
      "Processed 35801 lines.\r",
      "Processed 35901 lines.\r",
      "Processed 36001 lines.\r",
      "Processed 36101 lines.\r",
      "Processed 36201 lines.\r",
      "Processed 36301 lines.\r",
      "Processed 36401 lines.\r",
      "Processed 36501 lines.\r",
      "Processed 36601 lines.\r",
      "Processed 36701 lines.\r",
      "Processed 36801 lines.\r",
      "Processed 36901 lines.\r",
      "Processed 37001 lines.\r",
      "Processed 37101 lines.\r",
      "Processed 37201 lines.\r",
      "Processed 37301 lines.\r",
      "Processed 37401 lines.\r",
      "Processed 37501 lines.\r",
      "Processed 37601 lines.\r",
      "Processed 37701 lines.\r",
      "Processed 37801 lines.\r",
      "Processed 37901 lines.\r",
      "Processed 38001 lines.\r",
      "Processed 38101 lines.\r",
      "Processed 38201 lines.\r",
      "Processed 38301 lines.\r",
      "Processed 38401 lines.\r",
      "Processed 38501 lines.\r",
      "Processed 38601 lines.\r",
      "Processed 38701 lines.\r",
      "Processed 38801 lines.\r",
      "Processed 38901 lines.\r",
      "Processed 39001 lines.\r",
      "Processed 39101 lines.\r",
      "Processed 39201 lines.\r",
      "Processed 39301 lines.\r",
      "Processed 39401 lines.\r",
      "Processed 39501 lines.\r",
      "Processed 39601 lines.\r",
      "Processed 39701 lines.\r",
      "Processed 39801 lines.\r",
      "Processed 39901 lines.\r",
      "Processed 40001 lines.\r",
      "Processed 40101 lines.\r",
      "Processed 40201 lines.\r",
      "Processed 40301 lines.\r",
      "Processed 40401 lines.\r",
      "Processed 40501 lines.\r",
      "Processed 40601 lines.\r",
      "Processed 40701 lines.\r",
      "Processed 40801 lines.\r",
      "Processed 40901 lines.\r",
      "Processed 41001 lines.\r",
      "Processed 41101 lines.\r",
      "Processed 41201 lines.\r",
      "Processed 41301 lines.\r",
      "Processed 41401 lines.\r",
      "Processed 41501 lines.\r",
      "Processed 41601 lines.\r",
      "Processed 41701 lines.\r",
      "Processed 41801 lines.\r",
      "Processed 41901 lines.\r",
      "Processed 42001 lines.\r",
      "Processed 42101 lines.\r",
      "Processed 42201 lines.\r",
      "Processed 42301 lines.\r",
      "Processed 42401 lines.\r",
      "Processed 42501 lines.\r",
      "Processed 42601 lines.\r",
      "Processed 42701 lines.\r",
      "Processed 42801 lines.\r",
      "Processed 42901 lines.\r",
      "Processed 43001 lines.\r",
      "Processed 43101 lines.\r",
      "Processed 43201 lines.\r",
      "Processed 43301 lines.\r",
      "Processed 43401 lines.\r",
      "Processed 43501 lines.\r",
      "Processed 43601 lines.\r",
      "Processed 43701 lines.\r",
      "Processed 43801 lines.\r",
      "Processed 43901 lines.\r",
      "Processed 44001 lines.\r",
      "Processed 44101 lines.\r",
      "Processed 44201 lines.\r",
      "Processed 44301 lines.\r",
      "Processed 44401 lines.\r",
      "Processed 44501 lines.\r",
      "Processed 44601 lines.\r",
      "Processed 44701 lines.\r",
      "Processed 44801 lines.\r",
      "Processed 44901 lines.\r",
      "Processed 45001 lines.\r",
      "Processed 45101 lines.\r",
      "Processed 45201 lines.\r",
      "Processed 45301 lines.\r",
      "Processed 45401 lines.\r",
      "Processed 45501 lines.\r",
      "Processed 45601 lines.\r",
      "Processed 45701 lines.\r",
      "Processed 45801 lines.\r",
      "Processed 45901 lines.\r",
      "Processed 46001 lines.\r",
      "Processed 46101 lines.\r",
      "Processed 46201 lines.\r",
      "Processed 46301 lines.\r",
      "Processed 46401 lines.\r",
      "Processed 46501 lines.\r",
      "Processed 46601 lines.\r",
      "Processed 46701 lines.\r",
      "Processed 46801 lines.\r",
      "Processed 46901 lines.\r",
      "Processed 47001 lines.\r",
      "Processed 47101 lines.\r",
      "Processed 47201 lines.\r",
      "Processed 47301 lines.\r",
      "Processed 47401 lines.\r",
      "Processed 47501 lines.\r",
      "Processed 47601 lines.\r",
      "Processed 47701 lines.\r",
      "Processed 47801 lines.\r",
      "Processed 47901 lines.\r",
      "Processed 48001 lines.\r",
      "Processed 48101 lines.\r",
      "Processed 48201 lines.\r",
      "Processed 48301 lines.\r",
      "Processed 48401 lines.\r",
      "Processed 48501 lines.\r",
      "Processed 48601 lines.\r",
      "Processed 48701 lines.\r",
      "Processed 48801 lines.\r",
      "Processed 48901 lines.\r",
      "Processed 49001 lines.\r",
      "Processed 49101 lines.\r",
      "Processed 49201 lines.\r",
      "Processed 49301 lines.\r",
      "Processed 49401 lines.\r",
      "Processed 49501 lines.\r",
      "Processed 49601 lines.\r",
      "Processed 49701 lines.\r",
      "Processed 49801 lines.\r",
      "Processed 49901 lines.\r",
      "Processed 50001 lines.\r",
      "Processed 50101 lines.\r",
      "Processed 50201 lines.\r",
      "Processed 50301 lines.\r",
      "Processed 50401 lines.\r",
      "Processed 50501 lines.\r",
      "Processed 50601 lines.\r",
      "Processed 50701 lines.\r",
      "Processed 50801 lines.\r",
      "Processed 50901 lines.\r",
      "Processed 51001 lines.\r",
      "Processed 51101 lines.\r",
      "Processed 51201 lines.\r",
      "Processed 51301 lines.\r",
      "Processed 51401 lines.\r",
      "Processed 51501 lines.\r",
      "Processed 51601 lines.\r",
      "Processed 51701 lines.\r",
      "Processed 51801 lines.\r",
      "Processed 51901 lines.\r",
      "Processed 52001 lines.\r",
      "Processed 52101 lines.\r",
      "Processed 52201 lines.\r",
      "Processed 52301 lines.\r",
      "Processed 52401 lines.\r",
      "Processed 52501 lines.\r",
      "Processed 52601 lines.\r",
      "Processed 52701 lines.\r",
      "Processed 52801 lines.\r",
      "Processed 52901 lines.\r",
      "Processed 53001 lines.\r",
      "Processed 53101 lines.\r",
      "Processed 53201 lines.\r",
      "Processed 53301 lines.\r",
      "Processed 53401 lines.\r",
      "Processed 53501 lines.\r",
      "Processed 53601 lines.\r",
      "Processed 53701 lines.\r",
      "Processed 53801 lines.\r",
      "Processed 53901 lines.\r",
      "Processed 54001 lines.\r",
      "Processed 54101 lines.\r",
      "Processed 54201 lines.\r",
      "Processed 54301 lines.\r",
      "Processed 54401 lines.\r",
      "Processed 54501 lines.\r",
      "Processed 54601 lines.\r",
      "Processed 54701 lines.\r",
      "Processed 54801 lines.\r",
      "Processed 54901 lines.\r",
      "Processed 55001 lines.\r",
      "Processed 55101 lines.\r",
      "Processed 55201 lines.\r",
      "Processed 55301 lines.\r",
      "Processed 55401 lines.\r",
      "Processed 55501 lines.\r",
      "Processed 55601 lines.\r",
      "Processed 55701 lines.\r",
      "Processed 55801 lines.\r",
      "Processed 55901 lines.\r",
      "Processed 56001 lines.\r",
      "Processed 56101 lines.\r",
      "Processed 56201 lines.\r",
      "Processed 56301 lines.\r",
      "Processed 56401 lines.\r",
      "Processed 56501 lines.\r",
      "Processed 56601 lines.\r",
      "Processed 56701 lines.\r",
      "Processed 56801 lines.\r",
      "Processed 56901 lines.\r",
      "Processed 57001 lines.\r",
      "Processed 57101 lines.\r",
      "Processed 57201 lines.\r",
      "Processed 57301 lines.\r",
      "Processed 57401 lines.\r",
      "Processed 57501 lines.\r",
      "Processed 57601 lines.\r",
      "Processed 57701 lines.\r",
      "Processed 57801 lines.\r",
      "Processed 57901 lines.\r",
      "Processed 58001 lines.\r",
      "Processed 58101 lines.\r",
      "Processed 58201 lines.\r",
      "Processed 58301 lines.\r",
      "Processed 58401 lines.\r",
      "Processed 58501 lines.\r",
      "Processed 58601 lines.\r",
      "Processed 58701 lines.\r",
      "Processed 58801 lines.\r",
      "Processed 58901 lines.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58954/58954 [00:02<00:00, 27022.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 lines.\r",
      "Processed 101 lines.\r",
      "Processed 201 lines.\r",
      "Processed 301 lines.\r",
      "Processed 401 lines.\r",
      "Processed 501 lines.\r",
      "Processed 601 lines.\r",
      "Processed 701 lines.\r",
      "Processed 801 lines.\r",
      "Processed 901 lines.\r",
      "Processed 1001 lines.\r",
      "Processed 1101 lines.\r",
      "Processed 1201 lines.\r",
      "Processed 1301 lines.\r",
      "Processed 1401 lines.\r",
      "Processed 1501 lines.\r",
      "Processed 1601 lines.\r",
      "Processed 1701 lines.\r",
      "Processed 1801 lines.\r",
      "Processed 1901 lines.\r",
      "Processed 2001 lines.\r",
      "Processed 2101 lines.\r",
      "Processed 2201 lines.\r",
      "Processed 2301 lines.\r",
      "Processed 2401 lines.\r",
      "Processed 2501 lines.\r",
      "Processed 2601 lines.\r",
      "Processed 2701 lines.\r",
      "Processed 2801 lines.\r",
      "Processed 2901 lines.\r",
      "Processed 3001 lines.\r",
      "Processed 3101 lines.\r",
      "Processed 3201 lines.\r",
      "Processed 3301 lines.\r",
      "Processed 3401 lines.\r",
      "Processed 3501 lines.\r",
      "Processed 3601 lines.\r",
      "Processed 3701 lines.\r",
      "Processed 3801 lines.\r",
      "Processed 3901 lines.\r",
      "Processed 4001 lines.\r",
      "Processed 4101 lines.\r",
      "Processed 4201 lines.\r",
      "Processed 4301 lines.\r",
      "Processed 4401 lines.\r",
      "Processed 4501 lines.\r",
      "Processed 4601 lines.\r",
      "Processed 4701 lines.\r",
      "Processed 4801 lines.\r",
      "Processed 4901 lines.\r",
      "Processed 5001 lines.\r",
      "Processed 5101 lines.\r",
      "Processed 5201 lines.\r",
      "Processed 5301 lines.\r",
      "Processed 5401 lines.\r",
      "Processed 5501 lines.\r",
      "Processed 5601 lines.\r",
      "Processed 5701 lines.\r",
      "Processed 5801 lines.\r",
      "Processed 5901 lines.\r",
      "Processed 6001 lines.\r",
      "Processed 6101 lines.\r",
      "Processed 6201 lines.\r",
      "Processed 6301 lines.\r",
      "Processed 6401 lines.\r",
      "Processed 6501 lines.\r",
      "Processed 6601 lines.\r",
      "Processed 6701 lines.\r",
      "Processed 6801 lines.\r",
      "Processed 6901 lines.\r",
      "Processed 7001 lines.\r",
      "Processed 7101 lines.\r",
      "Processed 7201 lines.\r",
      "Processed 7301 lines.\r",
      "Processed 7401 lines.\r",
      "Processed 7501 lines.\r",
      "Processed 7601 lines.\r",
      "Processed 7701 lines.\r",
      "Processed 7801 lines.\r",
      "Processed 7901 lines.\r",
      "Processed 8001 lines.\r",
      "Processed 8101 lines.\r",
      "Processed 8201 lines.\r",
      "Processed 8301 lines.\r",
      "Processed 8401 lines.\r",
      "Processed 8501 lines.\r",
      "Processed 8601 lines.\r",
      "Processed 8701 lines.\r",
      "Processed 8801 lines.\r",
      "Processed 8901 lines.\r",
      "Processed 9001 lines.\r",
      "Processed 9101 lines.\r",
      "Processed 9201 lines.\r",
      "Processed 9301 lines.\r",
      "Processed 9401 lines.\r",
      "Processed 9501 lines.\r",
      "Processed 9601 lines.\r",
      "Processed 9701 lines.\r",
      "Processed 9801 lines.\r",
      "Processed 9901 lines.\r",
      "Processed 10001 lines.\r",
      "Processed 10101 lines.\r",
      "Processed 10201 lines.\r",
      "Processed 10301 lines.\r",
      "Processed 10401 lines.\r",
      "Processed 10501 lines.\r",
      "Processed 10601 lines.\r",
      "Processed 10701 lines.\r",
      "Processed 10801 lines.\r",
      "Processed 10901 lines.\r",
      "Processed 11001 lines.\r",
      "Processed 11101 lines.\r",
      "Processed 11201 lines.\r",
      "Processed 11301 lines.\r",
      "Processed 11401 lines.\r",
      "Processed 11501 lines.\r",
      "Processed 11601 lines.\r",
      "Processed 11701 lines.\r",
      "Processed 11801 lines.\r",
      "Processed 11901 lines.\r",
      "Processed 12001 lines.\r",
      "Processed 12101 lines.\r",
      "Processed 12201 lines.\r",
      "Processed 12301 lines.\r",
      "Processed 12401 lines.\r",
      "Processed 12501 lines.\r",
      "Processed 12601 lines.\r",
      "Processed 12701 lines.\r",
      "Processed 12801 lines.\r",
      "Processed 12901 lines.\r",
      "Processed 13001 lines.\r",
      "Processed 13101 lines.\r",
      "Processed 13201 lines.\r",
      "Processed 13301 lines.\r",
      "Processed 13401 lines.\r",
      "Processed 13501 lines.\r",
      "Processed 13601 lines.\r",
      "Processed 13701 lines.\r",
      "Processed 13801 lines.\r",
      "Processed 13901 lines.\r",
      "Processed 14001 lines.\r",
      "Processed 14101 lines.\r",
      "Processed 14201 lines.\r",
      "Processed 14301 lines.\r",
      "Processed 14401 lines.\r",
      "Processed 14501 lines.\r",
      "Processed 14601 lines.\r",
      "Processed 14701 lines.\r",
      "Processed 14801 lines.\r",
      "Processed 14901 lines.\r",
      "Processed 15001 lines.\r",
      "Processed 15101 lines.\r",
      "Processed 15201 lines.\r",
      "Processed 15301 lines.\r",
      "Processed 15401 lines.\r",
      "Processed 15501 lines.\r",
      "Processed 15601 lines.\r",
      "Processed 15701 lines.\r",
      "Processed 15801 lines.\r",
      "Processed 15901 lines.\r",
      "Processed 16001 lines.\r",
      "Processed 16101 lines.\r",
      "Processed 16201 lines.\r",
      "Processed 16301 lines.\r",
      "Processed 16401 lines.\r",
      "Processed 16501 lines.\r",
      "Processed 16601 lines.\r",
      "Processed 16701 lines.\r",
      "Processed 16801 lines.\r",
      "Processed 16901 lines.\r",
      "Processed 17001 lines.\r",
      "Processed 17101 lines.\r",
      "Processed 17201 lines.\r",
      "Processed 17301 lines.\r",
      "Processed 17401 lines.\r",
      "Processed 17501 lines.\r",
      "Processed 17601 lines.\r",
      "Processed 17701 lines.\r",
      "Processed 17801 lines.\r",
      "Processed 17901 lines.\r",
      "Processed 18001 lines.\r",
      "Processed 18101 lines.\r",
      "Processed 18201 lines.\r",
      "Processed 18301 lines.\r",
      "Processed 18401 lines.\r",
      "Processed 18501 lines.\r",
      "Processed 18601 lines.\r",
      "Processed 18701 lines.\r",
      "Processed 18801 lines.\r",
      "Processed 18901 lines.\r",
      "Processed 19001 lines.\r",
      "Processed 19101 lines.\r",
      "Processed 19201 lines.\r",
      "Processed 19301 lines.\r",
      "Processed 19401 lines.\r",
      "Processed 19501 lines.\r",
      "Processed 19601 lines.\r",
      "Processed 19701 lines.\r",
      "Processed 19801 lines.\r",
      "Processed 19901 lines.\r",
      "Processed 20001 lines.\r",
      "Processed 20101 lines.\r",
      "Processed 20201 lines.\r",
      "Processed 20301 lines.\r",
      "Processed 20401 lines.\r",
      "Processed 20501 lines.\r",
      "Processed 20601 lines.\r",
      "Processed 20701 lines.\r",
      "Processed 20801 lines.\r",
      "Processed 20901 lines.\r",
      "Processed 21001 lines.\r",
      "Processed 21101 lines.\r",
      "Processed 21201 lines.\r",
      "Processed 21301 lines.\r",
      "Processed 21401 lines.\r",
      "Processed 21501 lines.\r",
      "Processed 21601 lines.\r",
      "Processed 21701 lines.\r",
      "Processed 21801 lines.\r",
      "Processed 21901 lines.\r",
      "Processed 22001 lines.\r",
      "Processed 22101 lines.\r",
      "Processed 22201 lines.\r",
      "Processed 22301 lines.\r",
      "Processed 22401 lines.\r",
      "Processed 22501 lines.\r",
      "Processed 22601 lines.\r",
      "Processed 22701 lines.\r",
      "Processed 22801 lines.\r",
      "Processed 22901 lines.\r",
      "Processed 23001 lines.\r",
      "Processed 23101 lines.\r",
      "Processed 23201 lines.\r",
      "Processed 23301 lines.\r",
      "Processed 23401 lines.\r",
      "Processed 23501 lines.\r",
      "Processed 23601 lines.\r",
      "Processed 23701 lines.\r",
      "Processed 23801 lines.\r",
      "Processed 23901 lines.\r",
      "Processed 24001 lines.\r",
      "Processed 24101 lines.\r",
      "Processed 24201 lines.\r",
      "Processed 24301 lines.\r",
      "Processed 24401 lines.\r",
      "Processed 24501 lines.\r",
      "Processed 24601 lines.\r",
      "Processed 24701 lines.\r",
      "Processed 24801 lines.\r",
      "Processed 24901 lines.\r",
      "Processed 25001 lines.\r",
      "Processed 25101 lines.\r",
      "Processed 25201 lines.\r",
      "Processed 25301 lines.\r",
      "Processed 25401 lines.\r",
      "Processed 25501 lines.\r",
      "Processed 25601 lines.\r",
      "Processed 25701 lines.\r",
      "Processed 25801 lines.\r",
      "Processed 25901 lines.\r",
      "Processed 26001 lines.\r",
      "Processed 26101 lines.\r",
      "Processed 26201 lines.\r",
      "Processed 26301 lines.\r",
      "Processed 26401 lines.\r",
      "Processed 26501 lines.\r",
      "Processed 26601 lines.\r",
      "Processed 26701 lines.\r",
      "Processed 26801 lines.\r",
      "Processed 26901 lines.\r",
      "Processed 27001 lines.\r",
      "Processed 27101 lines.\r",
      "Processed 27201 lines.\r",
      "Processed 27301 lines.\r",
      "Processed 27401 lines.\r",
      "Processed 27501 lines.\r",
      "Processed 27601 lines.\r",
      "Processed 27701 lines.\r",
      "Processed 27801 lines.\r",
      "Processed 27901 lines.\r",
      "Processed 28001 lines.\r",
      "Processed 28101 lines.\r",
      "Processed 28201 lines.\r",
      "Processed 28301 lines.\r",
      "Processed 28401 lines.\r",
      "Processed 28501 lines.\r",
      "Processed 28601 lines.\r",
      "Processed 28701 lines.\r",
      "Processed 28801 lines.\r",
      "Processed 28901 lines.\r",
      "Processed 29001 lines.\r",
      "Processed 29101 lines.\r",
      "Processed 29201 lines.\r",
      "Processed 29301 lines.\r",
      "Processed 29401 lines.\r",
      "Processed 29501 lines.\r",
      "Processed 29601 lines.\r",
      "Processed 29701 lines.\r",
      "Processed 29801 lines.\r",
      "Processed 29901 lines.\r",
      "Processed 30001 lines.\r",
      "Processed 30101 lines.\r",
      "Processed 30201 lines.\r",
      "Processed 30301 lines.\r",
      "Processed 30401 lines.\r",
      "Processed 30501 lines.\r",
      "Processed 30601 lines.\r",
      "Processed 30701 lines.\r",
      "Processed 30801 lines.\r",
      "Processed 30901 lines.\r",
      "Processed 31001 lines.\r",
      "Processed 31101 lines.\r",
      "Processed 31201 lines.\r",
      "Processed 31301 lines.\r",
      "Processed 31401 lines.\r",
      "Processed 31501 lines.\r",
      "Processed 31601 lines.\r",
      "Processed 31701 lines.\r",
      "Processed 31801 lines.\r",
      "Processed 31901 lines.\r",
      "Processed 32001 lines.\r",
      "Processed 32101 lines.\r",
      "Processed 32201 lines.\r",
      "Processed 32301 lines.\r",
      "Processed 32401 lines.\r",
      "Processed 32501 lines.\r",
      "Processed 32601 lines.\r",
      "Processed 32701 lines.\r",
      "Processed 32801 lines.\r",
      "Processed 32901 lines.\r",
      "Processed 33001 lines.\r",
      "Processed 33101 lines.\r",
      "Processed 33201 lines.\r",
      "Processed 33301 lines.\r",
      "Processed 33401 lines.\r",
      "Processed 33501 lines.\r",
      "Processed 33601 lines.\r",
      "Processed 33701 lines.\r",
      "Processed 33801 lines.\r",
      "Processed 33901 lines.\r",
      "Processed 34001 lines.\r",
      "Processed 34101 lines.\r",
      "Processed 34201 lines.\r",
      "Processed 34301 lines.\r",
      "Processed 34401 lines.\r",
      "Processed 34501 lines.\r",
      "Processed 34601 lines.\r",
      "Processed 34701 lines.\r",
      "Processed 34801 lines.\r",
      "Processed 34901 lines.\r",
      "Processed 35001 lines.\r",
      "Processed 35101 lines.\r",
      "Processed 35201 lines.\r",
      "Processed 35301 lines.\r",
      "Processed 35401 lines.\r",
      "Processed 35501 lines.\r",
      "Processed 35601 lines.\r",
      "Processed 35701 lines.\r",
      "Processed 35801 lines.\r",
      "Processed 35901 lines.\r",
      "Processed 36001 lines.\r",
      "Processed 36101 lines.\r",
      "Processed 36201 lines.\r",
      "Processed 36301 lines.\r",
      "Processed 36401 lines.\r",
      "Processed 36501 lines.\r",
      "Processed 36601 lines.\r",
      "Processed 36701 lines.\r",
      "Processed 36801 lines.\r",
      "Processed 36901 lines.\r",
      "Processed 37001 lines.\r",
      "Processed 37101 lines.\r",
      "Processed 37201 lines.\r",
      "Processed 37301 lines.\r",
      "Processed 37401 lines.\r",
      "Processed 37501 lines.\r",
      "Processed 37601 lines.\r",
      "Processed 37701 lines.\r",
      "Processed 37801 lines.\r",
      "Processed 37901 lines.\r",
      "Processed 38001 lines.\r",
      "Processed 38101 lines.\r",
      "Processed 38201 lines.\r",
      "Processed 38301 lines.\r",
      "Processed 38401 lines.\r",
      "Processed 38501 lines.\r",
      "Processed 38601 lines.\r",
      "Processed 38701 lines.\r",
      "Processed 38801 lines.\r",
      "Processed 38901 lines.\r",
      "Processed 39001 lines.\r",
      "Processed 39101 lines.\r",
      "Processed 39201 lines.\r",
      "Processed 39301 lines.\r",
      "Processed 39401 lines.\r",
      "Processed 39501 lines.\r",
      "Processed 39601 lines.\r",
      "Processed 39701 lines.\r",
      "Processed 39801 lines.\r",
      "Processed 39901 lines.\r",
      "Processed 40001 lines.\r",
      "Processed 40101 lines.\r",
      "Processed 40201 lines.\r",
      "Processed 40301 lines.\r",
      "Processed 40401 lines.\r",
      "Processed 40501 lines.\r",
      "Processed 40601 lines.\r",
      "Processed 40701 lines.\r",
      "Processed 40801 lines.\r",
      "Processed 40901 lines.\r",
      "Processed 41001 lines.\r",
      "Processed 41101 lines.\r",
      "Processed 41201 lines.\r",
      "Processed 41301 lines.\r",
      "Processed 41401 lines.\r",
      "Processed 41501 lines.\r",
      "Processed 41601 lines.\r",
      "Processed 41701 lines.\r",
      "Processed 41801 lines.\r",
      "Processed 41901 lines.\r",
      "Processed 42001 lines.\r",
      "Processed 42101 lines.\r",
      "Processed 42201 lines.\r",
      "Processed 42301 lines.\r",
      "Processed 42401 lines.\r",
      "Processed 42501 lines.\r",
      "Processed 42601 lines.\r",
      "Processed 42701 lines.\r",
      "Processed 42801 lines.\r",
      "Processed 42901 lines.\r",
      "Processed 43001 lines.\r",
      "Processed 43101 lines.\r",
      "Processed 43201 lines.\r",
      "Processed 43301 lines.\r",
      "Processed 43401 lines.\r",
      "Processed 43501 lines.\r",
      "Processed 43601 lines.\r",
      "Processed 43701 lines.\r",
      "Processed 43801 lines.\r",
      "Processed 43901 lines.\r",
      "Processed 44001 lines.\r",
      "Processed 44101 lines.\r",
      "Processed 44201 lines.\r",
      "Processed 44301 lines.\r",
      "Processed 44401 lines.\r",
      "Processed 44501 lines.\r",
      "Processed 44601 lines.\r",
      "Processed 44701 lines.\r",
      "Processed 44801 lines.\r",
      "Processed 44901 lines.\r",
      "Processed 45001 lines.\r",
      "Processed 45101 lines.\r",
      "Processed 45201 lines.\r",
      "Processed 45301 lines.\r",
      "Processed 45401 lines.\r",
      "Processed 45501 lines.\r",
      "Processed 45601 lines.\r",
      "Processed 45701 lines.\r",
      "Processed 45801 lines.\r",
      "Processed 45901 lines.\r",
      "Processed 46001 lines.\r",
      "Processed 46101 lines.\r",
      "Processed 46201 lines.\r",
      "Processed 46301 lines.\r",
      "Processed 46401 lines.\r",
      "Processed 46501 lines.\r",
      "Processed 46601 lines.\r",
      "Processed 46701 lines.\r",
      "Processed 46801 lines.\r",
      "Processed 46901 lines.\r",
      "Processed 47001 lines.\r",
      "Processed 47101 lines.\r",
      "Processed 47201 lines.\r",
      "Processed 47301 lines.\r",
      "Processed 47401 lines.\r",
      "Processed 47501 lines.\r",
      "Processed 47601 lines.\r",
      "Processed 47701 lines.\r",
      "Processed 47801 lines.\r",
      "Processed 47901 lines.\r",
      "Processed 48001 lines.\r",
      "Processed 48101 lines.\r",
      "Processed 48201 lines.\r",
      "Processed 48301 lines.\r",
      "Processed 48401 lines.\r",
      "Processed 48501 lines.\r",
      "Processed 48601 lines.\r",
      "Processed 48701 lines.\r",
      "Processed 48801 lines.\r",
      "Processed 48901 lines.\r",
      "Processed 49001 lines.\r",
      "Processed 49101 lines.\r",
      "Processed 49201 lines.\r",
      "Processed 49301 lines.\r",
      "Processed 49401 lines.\r",
      "Processed 49501 lines.\r",
      "Processed 49601 lines.\r",
      "Processed 49701 lines.\r",
      "Processed 49801 lines.\r",
      "Processed 49901 lines.\r",
      "Processed 50001 lines.\r",
      "Processed 50101 lines.\r",
      "Processed 50201 lines.\r",
      "Processed 50301 lines.\r",
      "Processed 50401 lines.\r",
      "Processed 50501 lines.\r",
      "Processed 50601 lines.\r",
      "Processed 50701 lines.\r",
      "Processed 50801 lines.\r",
      "Processed 50901 lines.\r",
      "Processed 51001 lines.\r",
      "Processed 51101 lines.\r",
      "Processed 51201 lines.\r",
      "Processed 51301 lines.\r",
      "Processed 51401 lines.\r",
      "Processed 51501 lines.\r",
      "Processed 51601 lines.\r",
      "Processed 51701 lines.\r",
      "Processed 51801 lines.\r",
      "Processed 51901 lines.\r",
      "Processed 52001 lines.\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 52101 lines.\r",
      "Processed 52201 lines.\r",
      "Processed 52301 lines.\r",
      "Processed 52401 lines.\r",
      "Processed 52501 lines.\r",
      "Processed 52601 lines.\r",
      "Processed 52701 lines.\r",
      "Processed 52801 lines.\r",
      "Processed 52901 lines.\r",
      "Processed 53001 lines.\r",
      "Processed 53101 lines.\r",
      "Processed 53201 lines.\r",
      "Processed 53301 lines.\r",
      "Processed 53401 lines.\r",
      "Processed 53501 lines.\r",
      "Processed 53601 lines.\r",
      "Processed 53701 lines.\r",
      "Processed 53801 lines.\r",
      "Processed 53901 lines.\r",
      "Processed 54001 lines.\r",
      "Processed 54101 lines.\r",
      "Processed 54201 lines.\r",
      "Processed 54301 lines.\r",
      "Processed 54401 lines.\r",
      "Processed 54501 lines.\r",
      "Processed 54601 lines.\r",
      "Processed 54701 lines.\r",
      "Processed 54801 lines.\r",
      "Processed 54901 lines.\r",
      "Processed 55001 lines.\r",
      "Processed 55101 lines.\r",
      "Processed 55201 lines.\r",
      "Processed 55301 lines.\r",
      "Processed 55401 lines.\r",
      "Processed 55501 lines.\r",
      "Processed 55601 lines.\r",
      "Processed 55701 lines.\r",
      "Processed 55801 lines.\r",
      "Processed 55901 lines.\r",
      "Processed 56001 lines.\r",
      "Processed 56101 lines.\r",
      "Processed 56201 lines.\r",
      "Processed 56301 lines.\r",
      "Processed 56401 lines.\r",
      "Processed 56501 lines.\r",
      "Processed 56601 lines.\r",
      "Processed 56701 lines.\r",
      "Processed 56801 lines.\r",
      "Processed 56901 lines.\r",
      "Processed 57001 lines.\r",
      "Processed 57101 lines.\r",
      "Processed 57201 lines.\r",
      "Processed 57301 lines.\r",
      "Processed 57401 lines.\r",
      "Processed 57501 lines.\r",
      "Processed 57601 lines.\r",
      "Processed 57701 lines.\r",
      "Processed 57801 lines.\r",
      "Processed 57901 lines.\r",
      "Processed 58001 lines.\r",
      "Processed 58101 lines.\r",
      "Processed 58201 lines.\r",
      "Processed 58301 lines.\r",
      "Processed 58401 lines.\r",
      "Processed 58501 lines.\r",
      "Processed 58601 lines.\r",
      "Processed 58701 lines.\r",
      "Processed 58801 lines.\r",
      "Processed 58901 lines.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58954/58954 [10:24<00:00, 94.46it/s] \n"
     ]
    }
   ],
   "source": [
    "# Test Set Handling\n",
    "pathToSourceTestImages = os.path.join(pathToAllImages, \"test\")\n",
    "pathToTestLabels = os.path.join(pathToAllLabels, \"test_id_label.csv\")\n",
    "pathToDestTestImages = os.path.join(pathToAllResizedImages, \"test\")\n",
    "checkIfDirExistsAndCreate(pathToDestTestImages)\n",
    "pathToDestTestImages_2 = os.path.join(pathToAllResizedImages_2, \"test\")\n",
    "checkIfDirExistsAndCreate(pathToDestTestImages_2)\n",
    "\n",
    "\n",
    "imageListTuple = generateFileList(df_test_labels, pathToSourceTestImages, pathToDestTestImages, IMG_SIZES_1)\n",
    "resizeImagesMultiprocessing(imageListTuple)\n",
    "imageListTuple = generateFileList(df_test_labels, pathToSourceTestImages, pathToDestTestImages_2, IMG_SIZES_2)\n",
    "resizeImagesMultiprocessing(imageListTuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir not found, creating /home/armin/repos/FKD-Dataset/006_images_resized_2/val instead\n",
      "Processed 58901 lines.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58972/58972 [00:02<00:00, 28567.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 58901 lines.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58972/58972 [10:38<00:00, 92.39it/s] \n"
     ]
    }
   ],
   "source": [
    "# df_test_labels.head(1)\n",
    "\n",
    "# Val Set Handling\n",
    "pathToSourceValImages = os.path.join(pathToAllImages, \"val\")\n",
    "pathToValLabels = os.path.join(pathToAllLabels, \"val_id_label.csv\")\n",
    "pathToDestValImages = os.path.join(pathToAllResizedImages, \"val\")\n",
    "pathToDestValImages_2 = os.path.join(pathToAllResizedImages_2, \"val\")\n",
    "checkIfDirExistsAndCreate(pathToDestValImages)\n",
    "checkIfDirExistsAndCreate(pathToDestValImages_2)\n",
    "\n",
    "\n",
    "imageListTuple = generateFileList(df_val_labels, pathToSourceValImages, pathToDestValImages, IMG_SIZES_1)\n",
    "resizeImagesMultiprocessing(imageListTuple)\n",
    "imageListTuple = generateFileList(df_val_labels, pathToSourceValImages, pathToDestValImages_2, IMG_SIZES_2)\n",
    "resizeImagesMultiprocessing(imageListTuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir not found, creating /home/armin/repos/FKD-Dataset/007_text_image_label instead\n",
      "Dir not found, creating /home/armin/repos/FKD-Dataset/008_text_image_meta_label instead\n",
      "Dir not found, creating /home/armin/repos/FKD-Dataset/009_meta_label instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_to_cleaned_files_text_image_label_file = os.path.join(path_to_fakeddit_dataset_dir, \"007_text_image_label\")    \n",
    "checkIfDirExistsAndCreate(path_to_cleaned_files_text_image_label_file) \n",
    "\n",
    "path_to_cleaned_files_text_image_meta_label_file = os.path.join(path_to_fakeddit_dataset_dir, \"008_text_image_meta_label\")    \n",
    "checkIfDirExistsAndCreate(path_to_cleaned_files_text_image_meta_label_file) \n",
    "\n",
    "path_to_cleaned_meta_label_file = os.path.join(path_to_fakeddit_dataset_dir, \"009_meta_label\")    \n",
    "checkIfDirExistsAndCreate(path_to_cleaned_meta_label_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "createIDTitleTextLabelFile(df_taken_train, path_to_cleaned_files, 'train_title_image_label.csv')   \n",
    "createIDTitleTextLabelFile(df_taken_test, path_to_cleaned_files, 'test_title_image_label.csv')    \n",
    "createIDTitleTextLabelFile(df_taken_val, path_to_cleaned_files, 'val_title_image_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# df_taken_train = parallelize_dataframe_comments(df_taken_train, addComments1, 16)\n",
    "# df_taken_test = parallelize_dataframe(df_taken_test, addComments1, 16)\n",
    "# df_taken_val = parallelize_dataframe(df_taken_val, addComments1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "print('starting with comments train')\n",
    "createIDTitleCommentsTextLabelFile(df_taken_train, path_to_cleaned_files_text_image_label_file, 'train_text_image_label.csv', True)   \n",
    "\n",
    "print('starting with comments test')\n",
    "createIDTitleCommentsTextLabelFile(df_taken_test, path_to_cleaned_files_text_image_label_file, 'test_text_image_label.csv')    \n",
    "\n",
    "print('starting with comments val')\n",
    "createIDTitleCommentsTextLabelFile(df_taken_val, path_to_cleaned_files_text_image_label_file, 'val_text_image_label.csv')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "print('starting with all meta data train')\n",
    "createMetaDataLabelFile(df_taken_train, path_to_cleaned_meta_label_file, 'train_meta_label.csv')   \n",
    "\n",
    "print('starting with all meta data test')\n",
    "createMetaDataLabelFile(df_taken_test, path_to_cleaned_meta_label_file, 'test_meta_label.csv')    \n",
    "\n",
    "print('starting with all meta data val')\n",
    "createMetaDataLabelFile(df_taken_val, path_to_cleaned_meta_label_file, 'val_meta_label.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def addFullPath123(fileName, pathToImages):\n",
    "#     fileName = (str(fileName) + '.jpg')\n",
    "#     return os.path.join(pathToImages, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createIDTitleCommentsTextMetaDataLabelFile123(dataframe, pathToDirectory, pathToImages, fileName, isTrain = False): \n",
    "#     path_to_cleaned_csv = os.path.join(pathToDirectory, fileName)\n",
    "#     dataframe = dataframe.reindex(columns=['author_enc', 'clean_title', 'id', 'imagePath', 'comments', 'num_comments', 'up_vote_comments', 'score', 'hasNanScore', 'upvote_ratio', 'hasNanUpvote', '2_way_label'])\n",
    "#     dataframe['imagePath'] = dataframe['imagePath'].astype(str)\n",
    "#     if isTrain:\n",
    "# #         df = dataframe[['author_enc', 'clean_title', 'id', 'imagePath', 'score', 'hasNanScore', 'upvote_ratio', 'hasNanUpvote', 'comments', 'num_comments', 'up_vote_comments', 'means', 'stds', '2_way_label']]\n",
    "#         for tupleRaw in dataframe.itertuples(index=True, name=None):\n",
    "#             row_dict = convertRowToDictionary(tupleRaw, dataframe.columns, True)\n",
    "#             path = addFullPath123(row_dict['id'], pathToImages)\n",
    "#             dataframe.at[tupleRaw[0], \"imagePath\"] = path\n",
    "#     else:\n",
    "#         for tupleRaw in dataframe.itertuples(index=True, name=None):\n",
    "#             row_dict = convertRowToDictionary(tupleRaw, dataframe.columns, True)\n",
    "# #             df = dataframe[['author_enc', 'clean_title', 'id', 'imagePath', 'comments', 'num_comments', 'up_vote_comments', 'score', 'hasNanScore', 'upvote_ratio', 'hasNanUpvote', '2_way_label']]\n",
    "#             path = addFullPath123(row_dict['id'], pathToImages)\n",
    "#             dataframe.at[tupleRaw[0], \"imagePath\"] = path\n",
    "#     dataframe.to_csv(path_to_cleaned_csv, sep='\\t', encoding='utf-8', index=False)\n",
    "#     return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "print('starting with all data train')\n",
    "createIDTitleCommentsTextMetaDataLabelFile(df_taken_train, path_to_cleaned_files_text_image_meta_label_file, os.path.join(pathToAllResizedImages, 'train'), 'train_text_image_meta_label.csv', True)   \n",
    "\n",
    "print('starting with all data test')\n",
    "createIDTitleCommentsTextMetaDataLabelFile(df_taken_test, path_to_cleaned_files_text_image_meta_label_file, os.path.join(pathToAllResizedImages, 'test'),'test_text_image_meta_label.csv')    \n",
    "\n",
    "print('starting with all data val')\n",
    "createIDTitleCommentsTextMetaDataLabelFile(df_taken_val, path_to_cleaned_files_text_image_meta_label_file, os.path.join(pathToAllResizedImages, 'val'),'val_text_image_meta_label.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 591.5060166517893 minutes to process everything\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(f'It took {(end - start) / 60} minutes to process everything' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
